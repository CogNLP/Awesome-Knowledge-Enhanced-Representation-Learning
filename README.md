## Awesome Knowledge-Enhanced Natural Language Understanding

An awesome repository for knowledge-enhanced natural language understanding resources, including related papers, codes and datasets. Inspired by [KENLG-Reading](https://github.com/wyu97/KENLG-Reading).

<div align=center><img width="450" height="250" src="./figure/knowledge.png"/></div>


## Keywords Convention

![](https://img.shields.io/badge/Survey-blue) ![](https://img.shields.io/badge/Representation-orange) ![](https://img.shields.io/badge/Task-red)  ![](https://img.shields.io/badge/Knowledge-World-yellow)  ![](https://img.shields.io/badge/Knowledge-Linguistic-brightgreen)  ![](https://img.shields.io/badge/Knowledge-Commonsense-purple) 

## Basic NLU Papers for Beginners

+ Attention is All you Need, at NeurIPS 2017. \[[pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\]
+ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, on arXiv 2019. \[[pdf](https://arxiv.org/pdf/1810.04805.pdf)\]
+ RoBERTa: A Robustly Optimized BERT Pretraining Approach, on arXiv 2019. \[[pdf](https://arxiv.org/abs/1907.11692)\]
+ SpanBERT: Improving Pre-training by Representing and Predicting Spans, at TACL 2020. \[[pdf](https://watermark.silverchair.com/tacl_a_00300.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAswwggLIBgkqhkiG9w0BBwagggK5MIICtQIBADCCAq4GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMJSdEoxhDp2Vfkr4iAgEQgIICf3hP52SfBNJNCTxJA-VRo3ExIlGOLIddPfSj3IcpRWHHerFFqRl-orIB1rTw9CnQtr7OTZ-9JtUg0ut9qyj68gIut_7Atb8SU-rjzLGkeh2j0qpvYNzXOyJIdY-6d6E0zwWt1JPyV2G3fUIUfEtPRj0jl5ZfvyqdIq-nQG2eQM8L6A5y3mVsZx3LqNvNJSupvajaR9FmpyjWtgByta-dCnOahgnmuMGxFiPWqeYNrtawb18DycLI1bXwUHIMEvjrFUT1A_ouu7yvYovl8HQspQV-S2-Gl_RDwQ_wSSXgZznNiAaEva3C9Ajl7ZLJH3HXNUw3HQJSoBcntQmhWrylC-HIJ3BxSg5SfC9V4fhMY7ZzGkNKVOB0N6mziE-iBl57K8kezvcgPyACF9SDNNbgp3TO3CCN75etIdEMXSVpV17Ehut_HdK77lAfUcxspKE3KnRWsOfdh477xYdw-I5RVoSEtNtimj_YVw7KHGbEgBzZyMamqxhnqseElBoPOPpj67BX4NeN6kKteH1c4fzD00rDr834KYi5hi_5bXwxKCkc_nDBPdQp-3YolCGz5Z2WkEFj187JlkaftqJNf_NVhxYf1WTANc-7k5wMMXJlC-KNR60mieHWoCHQZIlwBuPQq-oI055JcGI4yI91objvBvCH8BOSYdACbnlGR_wsZxxfCFaNcYqaMqVY1t5OZFxqY2Q5ACRwzYu19GjFE-jPfXhGW84PF-NqsVpKJbJxywKfseHEpi8m86rP-eObzAlBiplhD9s9zkjy0pbM0P-ab5A2dOruqN1A_XDMOoKqEiKJIb_gbmaNHfk61HvzU4ZY3H8wDwsWQIRJRkaoJEBbLw)\]
+ A Primer in BERTology: What We Know About How BERT Works, at TACL 2020. \[[pdf](https://watermark.silverchair.com/tacl_a_00349.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsowggLGBgkqhkiG9w0BBwagggK3MIICswIBADCCAqwGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMwVFlFOPJnEtispYgAgEQgIICfW5LKhjUj8l71T16m9fhF8VSzMacFOPRdm00k2KBdpYkVpGAI8pGELEpSMatA_WHwzSiw3iwBSmcLaxqnHyxgsKMOiMu4VoakEUQJddgaLx_5l62e4BaksUPSaofHqo9RFMTq9pQJg3lPxiQhtn0xnyflTyR90kgq0gx4NlRWHxzYDHMvyGrsRlNGrmHDjcNTJcrN-vD9ER8Kw1dK9_BL9SLk4zb10cbfLLEAS4O5wQI6pSGml5WkRsbzHlej2GImj0wcoyPS9eTOHwg1w_yMj6h5Ey_e-18dN-g2CCwlNAtTefg9Cv-9O_CcQr9gBC7hn9scbUIcs2uemXtAslTiI2FiVDdcbqQ_cG44H5E3jRVVbGtMLkaqnqnt2D4APD98nWU0W9xUYzQsQ7rDKX_Ee5ShirJvSAFr_ZKFWxwI02twvFw8aWfGv89I5kkzmK6T2_QSQsNhSksZWCGvqDBIwz_MxiG-htsvIyP9rWlj2j4djGuv2QBhfzByIyO3D_ARkHG8FvxY-IeBiDhlP2e3i2eint_6-ss8oowDiE0XhllNa_CC-zKiCIeP8CixK2V77SmRBnV-ORYgp5A2Cjkjztmrt8YOdza29bdyiBCuYrg40uXoRmu26RVm3PizqNncMVHApRU6K5Glhhb3LDRlxRF_dOsyWiUxqm2EpbD4smJ-9htWm2RaHHkP_W5_lSmNuoEfLhsLlSImhMzMr0gL8lnZ3XBXJJNbMf2CWULUENAYt1-JHS_BB_-w8QujKxMp87oe9YPik92hueP25Mx82r_moEmdXtzswkqMleHBmic_PVyh8rGaV0Fb94EibAAHaB8s9rXdFQTIjfc1h8)\]

## KENLU Survey
- ![](https://img.shields.io/badge/Knowledge-World-yellow) Does Knowledge Help General NLU? An Empirical Study, on arXiv 2021. [[pdf](https://arxiv.org/pdf/2109.00563.pdf)]

## Knowledge-Enhanced Pre-training
+ ![](https://img.shields.io/badge/Survey-blue) A Survey of Knowledge Enhanced Pre-trained Models, on arXiv 2022. \[[pdf](https://arxiv.org/pdf/2110.00269.pdf)\]
+ **[Commonsense Knowledge]** Great Truths are Always Simple: A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models, at NAACL findings 2022. [[pdf](https://arxiv.org/pdf/2205.01841v1.pdf)] [[code](https://github.com/RUCAIBox/SAFE)]
+ ![](https://img.shields.io/badge/Survey-blue) Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey, on arXiv 2021. \[[pdf](https://arxiv.org/pdf/2110.08455.pdf)\]
+ ![](https://img.shields.io/badge/Survey-blue) Relational World Knowledge Representation in Contextual Language Models: A Review, at EMNLP 2021. \[[pdf](https://aclanthology.org/2021.emnlp-main.81.pdf)\]
+ ![](https://img.shields.io/badge/Survey-blue) Combining pre-trained language models and structured knowledge, on arXiv 2021. \[[pdf](https://arxiv.org/pdf/2101.12294.pdf)\]
+ ![](https://img.shields.io/badge/Survey-blue) Incorporating Extra Knowledge to Enhance Word Embedding, at IJCAI 2020. \[[pdf](https://dl.acm.org/doi/pdf/10.5555/3491440.3492126)\]
+ ![](https://img.shields.io/badge/Knowledge-Linguistic-brightgreen) Dict-BERT: Enhancing Language Model Pre-training with Dictionary, on arXiv 2022. \[[pdf](https://arxiv.org/pdf/2110.06490.pdf)\]
+  ![](https://img.shields.io/badge/Knowledge-World-yellow)  KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation, at TACL 2021. \[[pdf](https://watermark.silverchair.com/tacl_a_00360.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAswwggLIBgkqhkiG9w0BBwagggK5MIICtQIBADCCAq4GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMtMtrKwKiDSYsdiFFAgEQgIICf2Jm5LEeKelUFGKdPDgBZ7MLEo8AN3Q7AWgXeCQkCvOY1GWvND8D4eMG75qClzNId7IFKfG5mHytXSrTa7s3op86wBOV2BkRiptuM3WWWrMbX9dTed35hhZlYICCs66MnIhOy11_UWbowM-O-lZcxeR58ThAlkTdI3AmFesXh1EDl-vIyx9aMN7rbElb1TUJNxnf7IvqNhF9kKvejIyTmPhwWlSrKCRCTqH_OhoYPsHPcI1CPJCARf_dDF0qOfLWF6JtOoAckju8lZiXgPt1kRAumcZQU5GpS1VGL2CKjSZ8PnEUhdbh4NXfaZ4uHsAGNjqSVgfia_V3S4stdcEeonrLh-PYCCo3bApuzN7NklKA3FJPyQLmNoYqIuffzcuNOisPiWjLBNGisadvXgg9SXAP4bAWEisaHFifxe1pYzVUesyiH3R8yUutwaZBv4CvPkRSwDNlk5yT0LiwPaShuxg42qlsNOVAXdA0EkVAMeZQKsWaxZOEABAOEzebrVl2l862ctxcc3F31ElX8w7R-c4mhVUF8LCz2vScdXezt2are12Jf-yilJoWdfuJgt4HiaTI1M1TpLp9BeQoPr_OSSctzSknFNIO9oVRpuCBcnRbNiCT4g1EZvXZJjcSjsUlsdX85B6KB8-yNapBnk_Bri3ICzDyTN4YRsIAr_jOcuOJZWxridktd7UP2Faur5sgZQPz97AN2Py7XQrt-hH2wLWfD84w5-i5x5a7Qr426P1_3Ldaoj3mnFUOuq6G24mTfE0VzsVq4QomGFScjJQSSnYuySxOKbNGYb-5p_y_qLF71zKNP48Ay_d7IWeT1AR3KvaJRwTg3f9UdZvRSf3C3A)\]
+ **[Medical Knowledge]** SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining, at ACL 2021. \[[pdf](https://aclanthology.org/2021.acl-long.457.pdf)\] 
+  ![](https://img.shields.io/badge/Knowledge-World-yellow) Entities as Experts: Sparse Memory Asccess with Entity Supervision, at EMNLP 2020. \[[pdf](https://aclanthology.org/2020.emnlp-main.400.pdf)\] 
+ **[Commonsense Knowledge]** Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models, on arXiv 2020. \[[pdf](https://arxiv.org/pdf/1908.06725.pdf)\]

## Knowledge-Enhanced Text Representation
- **[Entity Representation]** mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models, at ACL 2022. \[[pdf](https://arxiv.org/pdf/2110.08151.pdf)\]
- Infusing Finetuning with Semantic Dependencies, at TACL 2021. \[[pdf](https://watermark.silverchair.com/tacl_a_00363.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsowggLGBgkqhkiG9w0BBwagggK3MIICswIBADCCAqwGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM1ZfV-8kcvj_3eUa2AgEQgIICfV-aC25MhnX6CA1RZETkcpeu_tRPQttd_5mq542f-SzNa2yFe4ElTYpv-trg96F86ib8tri4EeWeNm6e4P85Kzf69gD_Oj2qtFD438MuEa_Nk_NfgkM-AxO0O296Ro0Yhyr01z9oAG2nf7SaNq2298MEDjIkUcLb32V07IccUCcHRI0lXY4QmDyYWz4LazkXOVa3zZ4bLCxR7Fky-CiDI6jDTFWFf8bEBg9Ik_srp1CzjcQfkymXXevwdWGXAqxUvF8LBJFCm0ejDEePueN8lHoF4AYPd9JvEqr2WUGnAm1aDxsPfYKB-6s3s4Q_AVLqH4GzQEN0XJo9uPdeQn393_zG1zcJW05eXl51HqhxxplvR_fwmm8Rblajgpu-NwuqIy_Nm1rerobdwV7MbqsnTXQSIPymD82dfu_v0X9fJ2sZ6OkTSbijoiRtfnCgG0aHVGC2XOGAaK45U1XfUuvjqjMjWIgGXifRCWjRBLIoqlBOOd7km4BKaoLAKHj9sBr5-1XxTPzmZXN4VMvq52pxNWqQ33s1fPciQuTatioRC3g6uPOgWG-Hr9_mp8SvBvCQeY_idNMeY5kbgZmxrIrpO675LWMPSAwOMxDP5GT4LcWRDaUi7vDUwpqLQwYwYuzbpoaqXgCXtD5RvRWSBSHK8hpso1j8dY-nWsdjDfrM90WC7c8EKcPAylELBqzvOgk2dtssp2wkoXTozFsEjkTg0treXXt5jhogJ0CNe6NKuRo9E72dbou9H281W-WVDFmZTU5VCAKUcLdfY7elkaSyY5QlBM4StOEWAIdYPLQcu6Y6Z0x2S_odw9KmDQS1mogkMKr3JAXLtMwQionihYk)\]
- Biomedical Interpretable Entity Representations, at ACL findings 2021. \[[pdf](https://aclanthology.org/2021.findings-acl.311.pdf)\]
- **[Event Representation]** GENE: Global Event Network Embedding, at TextGraphs 2021. \[[pdf](https://aclanthology.org/2021.textgraphs-1.5.pdf)\]
- Incorporating Extra Knowledge to Enhance Word Embedding, at IJCAI 2021. \[[pdf](https://www.ijcai.org/proceedings/2020/0686.pdf)\]
- **[Entity Representation]** LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention, at EMNLP 2020. \[[pdf](https://aclanthology.org/2020.emnlp-main.523.pdf)\]
- Interpretable Entity Representations through Large-Scale Typing, at EMNLP findings 2020. \[[pdf](https://aclanthology.org/2020.findings-emnlp.54.pdf)\]
- Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information, at ACL 2020. [[pdf](https://aclanthology.org/2020.acl-main.255.pdf)]
- ![](https://img.shields.io/badge/Knowledge-Linguistic-brightgreen) Semantics-Aware BERT for Language Understanding, at AAAI 2020. \[[pdf](https://arxiv.org/abs/1909.02209)\]
- Knowledge Enhanced Contextual Word Representations, at EMNLP 2019. \[[pdf](https://aclanthology.org/D19-1005.pdf)\]
- **[Event Representation]** **[Commonsense Knowledge]** Event Representation Learning Enhanced with External Commonsense Knowledge, at EMNLP 2019. \[[pdf](https://aclanthology.org/D19-1495.pdf)\]
- Offline versus Online Representation Learning of Documents Using External Knowledge, at TOIS 2019. \[[pdf](https://dl.acm.org/doi/abs/10.1145/3349527?casa_token=FQdaxs1gjN8AAAAA:YqIzQLc4hL-4mQZfevD6FV_FCF2JH2sCpKns49WpuqTRwPBHO_oqfhhESj5Kgb9LHGXjIu59yIalkg)\]

## Knowledge-Enhanced Text Classification

+ Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification, at ACL 2022. [[pdf](https://arxiv.org/pdf/2108.02035.pdf)]
+ KenMeSH: Knowledge-enhanced End-to-end Biomedical Text Labelling, on arXiv 2022. [[pdf](https://arxiv.org/pdf/2203.06835.pdf)]
+ KESA: A Knowledge Enhanced Approach For Sentiment Analysis, on arXiv 2022. \[[pdf](https://arxiv.org/pdf/2202.12093.pdf)\]
+ ![](https://img.shields.io/badge/Knowledge-World-yellow) Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge, at ACL 2021. [[pdf](https://aclanthology.org/2021.acl-long.62.pdf)] [[code](https://github.com/BUPT-GAMMA/CompareNet_FakeNewsDetection)]
+ **[Entity Knowledge]**  **[Fact Verification]** Modeling Entity Knowledge for Fact Verification, at FEVER 2021. [[pdf](https://aclanthology.org/2021.fever-1.6.pdf)]
+ Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification, at EMNLP 2021. [[pdf](https://aclanthology.org/2021.emnlp-main.247.pdf)]
+ Knowledge-Guided Paraphrase Identification, at EMNLP findings 2021. [[pdf](https://aclanthology.org/2021.findings-emnlp.72.pdf)]
+ KinGDOM: Knowledge-Guided DOMain adaptation for sentiment analysis, at ACL 2020. [[pdf](https://aclanthology.org/2020.acl-main.292.pdf)]

## Knowledge-Enhanced Information Extraction
- Enhanced Language Representation with Label Knowledge for Span Extraction, at EMNLP 2021. [[pdf](https://aclanthology.org/2021.emnlp-main.379.pdf)] 
- Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks, at ACL 2021. [[pdf](https://aclanthology.org/2021.acl-long.376.pdf)]
- Fine-grained Information Extraction from Biomedical Literature based on Knowledge-enriched Abstract Meaning Representation, at ACL 2021. [[pdf](https://aclanthology.org/2021.acl-long.489.pdf)]
- Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference, at ACL 2021. [[pdf](https://aclanthology.org/2021.acl-long.488.pdf)]
- Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter, at ACL 2021. [[pdf](https://aclanthology.org/2021.acl-long.454v2.pdf)]
- **[Event Detection]** Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event Detection, at ACL findings 2021. [[pdf](https://aclanthology.org/2021.findings-acl.214.pdf)]
- **[Named Entity Recognition]** Knowledge-aware Named Entity Recognition with Alleviating Heterogeneity, at AAAI 2021. [[pdf](https://www.aaai.org/AAAI21Papers/AAAI-9155.NieB.pdf)]
- Metadata Shaping: Natural Language Annotations for the Tail, on arXiv 2021. [[pdf](https://arxiv.org/pdf/2110.08430.pdf)]
- **[Event Causality Identification]** Knowledge Enhanced Event Causality Identification with Mention Masking Generalizations, at IJCAI 2020. [[pdf](https://www.ijcai.org/proceedings/2020/0499.pdf)] 
- **[Named Entity Recognition]** Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph Network, at EMNLP 2019. [[pdf](https://aclanthology.org/D19-1396.pdf)] 
- **[Relation Extraction]** Improving Relation Extraction with Knowledge-attention, at EMNLP 2019. [[pdf](https://aclanthology.org/D19-1022.pdf)] 

## Knowledge-Enhanced Semantics and Syntax Parsing
- A Unified Syntax-aware Framework for Semantic Role Labeling, at EMNLP 2018. [[pdf](https://aclanthology.org/D18-1262.pdf)]


## Knowledge-Enhanced Information Retrieval
- LET: Linguistic Knowledge Enhanced Graph Transformer for Chinese Short Text Matching, at AAAI 2021. [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/17592)]
-  Entity-aware Transformers for Entity Search,on arXiv 2022.[[pdf](https://arxiv.org/pdf/2205.00820v1.pdf)]
- **[Recommender System]** KRED: Knowledge-Aware Document Representation for News Recommendations, at RecSys 2020. [[pdf](https://arxiv.org/pdf/1910.11494.pdf)]
- Learning Unsupervised Knowledge-Enhanced Representations to Reduce the Semantic Gap in Information Retrieval, at TIOS 2020. [[pdf](https://dl.acm.org/doi/abs/10.1145/3417996)]
- Knowledge Enhanced Hybrid Neural Network for Text Matching, at AAAI 2018. [[pdf](https://openreview.net/forum?id=ryZBFeZ_-H)]
- **[Recommender System]** DKN: Deep Knowledge-Aware Network for News Recommendation, at WWW 2018. [[pdf](https://dl.acm.org/doi/pdf/10.1145/3178876.3186175)]

## Knowledge-Enhanced Machine Reading Comprehension
+  ![](https://img.shields.io/badge/Knowledge-Linguistic-brightgreen) SG-Net: Syntax-Guided Machine Reading Comprehension, at AAAI 2020. [[pdf](https://arxiv.org/pdf/1908.05147.pdf)]
+ Incorporating Syntax and Frame Semantics in Neural Network for Machine Reading Comprehension, at COLING 2020. [[pdf](https://aclanthology.org/2020.coling-main.237/)]
+ Machine Reading Comprehension Using Structural Knowledge Graph-aware Network, at EMNLP 2019. [[pdf](https://aclanthology.org/D19-1602.pdf)]
+ Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge, at EMNLP 2018. [[pdf](https://aclanthology.org/P18-1076.pdf)]

## Knowledge-Enhanced Question Answering

+ Instilling Type Knowledge in Language Models via Multi-Task QA, on arXiv 2022.[[pdf](https://arxiv.org/pdf/2204.13796v1.pdf)]

+ Unstructured Text Enhanced Open-Domain Dialogue System: A Systematic Survey, at TOIS 2022. [[pdf](https://dl.acm.org/doi/abs/10.1145/3464377?casa_token=y4ngVLYx2WsAAAAA:mu1ibata4LRdqibfIQ0iyecpzA5gEdz7WJYoLOFebukQg7mPvU5GDTaZz9GfL7Ce5PuGgnY9_67sVQ)]
+ Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition, at EMNLP 2020. [[pdf](https://aclanthology.org/2020.emnlp-main.372.pdf)]
+ Knowledge-aware Attentive Neural Network for Ranking Question Answer Pairs, at SIGIR 2018. [[pdf](https://dl.acm.org/doi/pdf/10.1145/3209978.3210081)]
+ Dynamic Integration of Background Knowledge in Neural NLU Systems, on arXiv 2018. [[pdf](https://arxiv.org/pdf/2010.04389.pdf)]


## External Knowledge Memory

+ Mention Memory: incorporating textual knowledge into Transformers through entity mention attention, at ICLR 2022. [[pdf](https://openreview.net/pdf?id=OY1A8ejQgEX)]
+ A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models, at ACL 2022. [[pdf](https://arxiv.org/pdf/2202.13392.pdf)]


## Knowledge-Related Datasets

+ CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge, at NeurIPS 2021. \[[pdf](https://openreview.net/pdf?id=mbW_GT3ZN-)\]
+ GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding, at EMNLP 2018. \[[pdf](https://aclanthology.org/W18-5446.pdf)\]



## Other Related Projects
- ![](https://img.shields.io/badge/Survey-blue) **[Prompt]** Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing, on arXiv 2021. [[pdf](https://arxiv.org/pdf/2107.13586.pdf)]
- Knowledge-enriched Text Generation Survey, Tutorial and Reading, on github. [[home](https://github.com/wyu97/KENLG-Reading)]
- A Survey of Knowledge-Enhanced Text Generation, on arXiv 2020. [[pdf](https://arxiv.org/pdf/2010.04389.pdf)]
- PromptPapers, on github. [[home](https://github.com/thunlp/PromptPapers)]
- RetrivalLMPapers, on github. [[home](https://github.com/Timothyxxx/RetrivalLMPapers)]


